{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Token var=<ContextVar name='format_options' default={'edgeitems': 3, 'threshold': 1000, 'floatmode': 'maxprec', 'precision': 8, 'suppress': False, 'linewidth': 75, 'nanstr': 'nan', 'infstr': 'inf', 'sign': '-', 'formatter': None, 'legacy': 9223372036854775807, 'override_repr': None} at 0x73bdbe597970> at 0x73bd25f93bc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from pprint import pprint\n",
    "import tqdm\n",
    "\n",
    "from pymdp.utils import plot_beliefs, plot_likelihood\n",
    "from pymdp import utils\n",
    "from pymdp.envs import TMazeEnv, MultiArmedBanditEnv\n",
    "from pymdp.pdo_agents import PDOAgentGradient, EVAgentGradient, EVAgentDirect\n",
    "from pymdp.agent import Agent\n",
    "\n",
    "np.set_printoptions(linewidth=60, precision=3, suppress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward_probabilities = [0.98, 0.02] # probabilities used in the original SPM T-maze demo\n",
    "# reward_probabilities = [0.5, 0.5] # probabilities used in the original SPM T-maze demo\n",
    "# env = TMazeEnv(reward_probs = reward_probabilities)·\n",
    "# env = MultiArmedBanditEnv(4)\n",
    "# env.values = [1.0, 2.0, -1.0, 0.0, 0.0] # The last one is the initial state value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EV - direct\n",
      "\n",
      "Using <class 'pymdp.pdo_agents.agent_direct.EVAgentDirect'>\n",
      "(4, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling runs: 100%|██████████| 100/100 [00:00<00:00, 701.52it/s, Umean=0.8, Ustd=2.22e-16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: [(np.float64(0.8), 100)]\n",
      "Mean reward of 100: 0.8, std=2.22e-16\n",
      "Location sequences: [('cL:0-C-L', 51), ('cR:0-C-R', 49)]\n",
      "\"Stats: {'EV': np.float64(0.8), 'G': np.float64(-0.8), 'nodes': 14}\"\n",
      "\n",
      "EV - GD\n",
      "\n",
      "Using <class 'pymdp.pdo_agents.agent_gradient.EVAgent'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 191.81it/s, G=-0.7947341]\n",
      "Sampling runs: 100%|██████████| 100/100 [00:00<00:00, 702.62it/s, Umean=0.778, Ustd=0.155]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: [(np.float64(0.8), 98), (np.float64(-0.2), 1), (np.float64(-0.4), 1)]\n",
      "Mean reward of 100: 0.778, std=0.155\n",
      "Location sequences: [('cR:0-C-R', 54), ('cL:0-C-L', 44), ('cL:0-C-0', 1), ('cL:0-C-C', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reward_probabilities = [1.0, 0.0]\n",
    "# reward_probabilities = [0.98, 0.02] # probabilities used in the original SPM T-maze demo\n",
    "env = TMazeEnv(reward_probs=reward_probabilities)\n",
    "\n",
    "# these are useful for displaying read-outs during the loop over time\n",
    "reward_conditions = [\"Right\", \"Left\"]\n",
    "location_observations = ['CENTER', 'RIGHT ARM', 'LEFT ARM', 'CUE LOCATION']\n",
    "location_codes = \"0RLC\"\n",
    "reward_observations = ['No reward', 'Reward!', 'Loss!']\n",
    "cue_observations = ['Cue Right', 'Cue Left']\n",
    "\n",
    "\n",
    "def obs_text(obs):\n",
    "    return str((location_observations[obs[0]], reward_observations[obs[1]], cue_observations[obs[2]]))\n",
    "\n",
    "\n",
    "def counts(vals):\n",
    "    counts = {}\n",
    "    for v in vals:\n",
    "        counts.setdefault(v, 0)\n",
    "        counts[v] += 1\n",
    "    return sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def run_experiment(env, agent_init_func, reps, steps, progress=True, reinit_agent=True, show_reps=0):\n",
    "    rews = []\n",
    "    loc_obs = []\n",
    "    if not reinit_agent:\n",
    "        # Initialize the agent only once\n",
    "        agent = agent_init_func()\n",
    "        # Let it infer the complete policy (if e.g. PDO agent)\n",
    "        agent.infer_states(env.reset())\n",
    "        agent.infer_policies()\n",
    "\n",
    "    for rep in (bar := tqdm.trange(reps, desc=\"Sampling runs\", disable=not progress, leave=True)):\n",
    "        if reinit_agent:\n",
    "            agent = agent_init_func()\n",
    "        else:\n",
    "            agent.reset()\n",
    "\n",
    "        obs = env.reset()  # reset the environment and get an initial observation\n",
    "        rew = 0.0\n",
    "        loc_ob = f\"c{'RL'[env.reward_condition]}:{location_codes[obs[0]]}\"\n",
    "\n",
    "        if rep < show_reps:\n",
    "            bar.write(f\"=== New run ({\n",
    "                      reward_conditions[env.reward_condition]}, observation: {obs_text(obs)} ===\")\n",
    "\n",
    "        for t in range(steps):\n",
    "            agent.infer_states(obs)\n",
    "            agent.infer_policies()\n",
    "            action = agent.sample_action()\n",
    "            obs = env.step(action)\n",
    "            if rep < show_reps:\n",
    "                bar.write(f\"[Step {t}] action: [Move to {\n",
    "                          location_observations[int(action[0])]}], observation: {obs_text(obs)}\")\n",
    "\n",
    "            for i in range(len(agent.C)):\n",
    "                rew += agent.C[i][obs[i]]\n",
    "            loc_ob = f\"{loc_ob}-{location_codes[obs[0]]}\"\n",
    "\n",
    "        rews.append(float(rew))\n",
    "        loc_obs.append(loc_ob)\n",
    "        bar.set_postfix(Umean=np.mean(rews), Ustd=np.std(rews))\n",
    "    bar.close()\n",
    "\n",
    "    rews = np.array(rews)\n",
    "\n",
    "    print(f\"Rewards: {counts(rews)}\")\n",
    "    print(f\"Mean reward of {reps}: {np.mean(rews):.3}, std={np.std(rews):.3}\")\n",
    "    print(f\"Location sequences: {counts(loc_obs)}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "T_CLUE = -0.2\n",
    "T_WIN = 1.0\n",
    "T_LOSS = -3.0\n",
    "\n",
    "PDO_BETA = 1000.0\n",
    "PDO_ITERATIONS = 1000\n",
    "PDO_LR = 1.\n",
    "PDO_EV = False\n",
    "PDO_DIRECT = True\n",
    "EFE_SOPHISTICATED = False\n",
    "\n",
    "REPS = 100\n",
    "T = 2\n",
    "\n",
    "\n",
    "def init_efe():\n",
    "    A_gm = copy.deepcopy(env.get_likelihood_dist())\n",
    "    B_gm = copy.deepcopy(env.get_transition_dist())\n",
    "    agent = Agent(A=A_gm, B=B_gm, control_fac_idx=[\n",
    "                  0], sophisticated=EFE_SOPHISTICATED)\n",
    "    agent.D[0] = utils.onehot(0, agent.num_states[0])\n",
    "    agent.C[1][1] = T_WIN\n",
    "    agent.C[1][2] = T_LOSS\n",
    "    agent.C[0][3] = T_CLUE\n",
    "    return agent\n",
    "\n",
    "\n",
    "def init_pdo():\n",
    "    A_gm = copy.deepcopy(env.get_likelihood_dist())\n",
    "    B_gm = copy.deepcopy(env.get_transition_dist())\n",
    "    if PDO_EV:\n",
    "        if PDO_DIRECT:\n",
    "            agent = EVAgentDirect(A=A_gm, B=B_gm, time_horizon=T, env=env,\n",
    "                                  policy_lr=PDO_LR, beta=PDO_BETA, policy_iterations=PDO_ITERATIONS)\n",
    "        else:\n",
    "            agent = EVAgentGradient(A=A_gm, B=B_gm, time_horizon=T, env=env,\n",
    "                            policy_lr=PDO_LR, beta=PDO_BETA, policy_iterations=PDO_ITERATIONS)\n",
    "    else:\n",
    "        if PDO_DIRECT:\n",
    "            assert 0, \"Direct PDO not implemented\"\n",
    "        else:\n",
    "            agent = PDOAgentGradient(A=A_gm, B=B_gm, time_horizon=T, env=env,\n",
    "                             policy_lr=PDO_LR, beta=PDO_BETA, policy_iterations=PDO_ITERATIONS)\n",
    "    print(f\"Using {type(agent)}\")\n",
    "    agent.D[0] = utils.onehot(0, agent.num_states[0])\n",
    "    agent.C[1][1] = T_WIN\n",
    "    agent.C[1][2] = T_LOSS\n",
    "    agent.C[0][3] = T_CLUE\n",
    "    return agent\n",
    "\n",
    "\n",
    "PDO_EV = True\n",
    "PDO_DIRECT = True\n",
    "print(f\"\\nEV - direct\\n\")\n",
    "ag = run_experiment(env, init_pdo, REPS, steps=T,\n",
    "                    show_reps=0, reinit_agent=False)\n",
    "pprint(f\"Stats: {ag.stats}\")\n",
    "\n",
    "# PDO_EV = False\n",
    "# PDO_DIRECT = True\n",
    "# print(f\"\\nPDO - direct\\n\")\n",
    "# run_experiment(env, init_pdo, REPS, steps=T, show_reps=0, reinit_agent=False)\n",
    "# pprint(f\"Stats: {ag.stats}\")\n",
    "\n",
    "PDO_EV = True\n",
    "PDO_DIRECT = False\n",
    "print(f\"\\nEV - GD\\n\")\n",
    "ag = run_experiment(env, init_pdo, REPS, steps=T,\n",
    "                    show_reps=0, reinit_agent=False)\n",
    "\n",
    "# PDO_EV = False\n",
    "# PDO_DIRECT = False\n",
    "# print(f\"\\nPDO - GD\\n\")\n",
    "# run_experiment(env, init_pdo, REPS, steps=T, show_reps=0, reinit_agent=False)\n",
    "\n",
    "# EFE_SOPHISTICATED = False\n",
    "# print(f\"\\nEFE (soph={EFE_SOPHISTICATED})\\n\")\n",
    "# run_experiment(env, init_efe, REPS, steps=T, show_reps=0, reinit_agent=True)\n",
    "\n",
    "# EFE_SOPHISTICATED = True\n",
    "# print(f\"\\nEFE (soph={EFE_SOPHISTICATED})\\n\")\n",
    "# run_experiment(env, init_efe, REPS, steps=T, show_reps=0, reinit_agent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# agent = PDOAgent(A=A_gm, B=B_gm, time_horizon=T, env=env,\n",
    "#                  policy_lr=10., beta=10.0, policy_iterations=500)\n",
    "# obs = env.reset()  # reset the environment and get an initial observation\n",
    "\n",
    "# print(f\"Consistent observation sequences: {len(agent.generate_consistent_observation_seqs(\n",
    "# ))} (out of approx {len(agent.possible_observations) ** T})\")\n",
    "\n",
    "# if isinstance(env, TMazeEnv):\n",
    "#     agent.D[0] = utils.onehot(0, agent.num_states[0])\n",
    "#     agent.C[1][1] = 1.0\n",
    "#     agent.C[1][2] = -1.0\n",
    "#     agent.C[0][3] = -T_CLUE_PENALTY\n",
    "\n",
    "#     # these are useful for displaying read-outs during the loop over time\n",
    "#     reward_conditions = [\"Right\", \"Left\"]\n",
    "#     location_observations = ['CENTER', 'RIGHT ARM', 'LEFT ARM', 'CUE LOCATION']\n",
    "#     reward_observations = ['No reward', 'Reward!', 'Loss!']\n",
    "#     cue_observations = ['Cue Right', 'Cue Left']\n",
    "\n",
    "#     rews = []\n",
    "#     for reps in range(REPS):\n",
    "#         rew = 0.0\n",
    "#         agent.reset()\n",
    "#         obs = env.reset()  # reset the environment and get an initial observation\n",
    "#         if reps < REPS_SHOW:\n",
    "#             msg = \"\"\" === Starting experiment === \\n Reward condition: {}, Observation: [{}, {}, {}]\"\"\"\n",
    "#             print(msg.format(reward_conditions[env.reward_condition],\n",
    "#                   location_observations[obs[0]], reward_observations[obs[1]], cue_observations[obs[2]]))\n",
    "#         for t in range(T):\n",
    "#             agent.infer_states(obs)\n",
    "#             agent.infer_policies()\n",
    "#             action = agent.sample_action()\n",
    "\n",
    "#             if reps < REPS_SHOW:\n",
    "#                 msg = \"\"\"[Step {}] Action: [Move to {}]\"\"\"\n",
    "#                 print(msg.format(t, location_observations[int(action[0])]))\n",
    "#             obs = env.step(action)\n",
    "#             if reps < REPS_SHOW:\n",
    "#                 msg = \"\"\"[Step {}] Observation: [{},  {}, {}]\"\"\"\n",
    "#                 print(msg.format(\n",
    "#                     t, location_observations[obs[0]], reward_observations[obs[1]], cue_observations[obs[2]]))\n",
    "\n",
    "#             rew += efe_agent.C[0][obs[0]]\n",
    "#             rew += efe_agent.C[1][obs[1]]\n",
    "#             rew += efe_agent.C[2][obs[2]]\n",
    "\n",
    "#         rews.append(rew)\n",
    "\n",
    "#     print(f\"Mean reward (of {REPS}): {np.mean(rews)}\")\n",
    "\n",
    "# elif isinstance(env, MultiArmedBanditEnv):\n",
    "#     agent.D[0] = utils.onehot(env.INITIAL_STATE, agent.num_states[0])\n",
    "#     agent.C[1][:] = env.values\n",
    "\n",
    "#     print(f\"Initial observation: {obs}\")\n",
    "#     for t in range(T):\n",
    "#         agent.infer_states(obs)\n",
    "#         agent.infer_policies()\n",
    "#         action = agent.sample_action()\n",
    "#         obs = env.step(action)\n",
    "#         print(f\"Action: {np.array(action)}, Observation: {\n",
    "#               np.array(obs)}, Reward: {env.values[obs[0]]}\")\n",
    "\n",
    "# else:\n",
    "#     raise ValueError(\"Unknown environment type\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
