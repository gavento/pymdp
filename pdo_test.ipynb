{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from pymdp.utils import plot_beliefs, plot_likelihood\n",
    "from pymdp import utils\n",
    "from pymdp.envs import TMazeEnv, MultiArmedBanditEnv\n",
    "from pymdp.pdo_agent import PDOAgent, EVAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_probabilities = [0.98, 0.02] # probabilities used in the original SPM T-maze demo\n",
    "# reward_probabilities = [0.5, 0.5] # probabilities used in the original SPM T-maze demo\n",
    "env = TMazeEnv(reward_probs = reward_probabilities)\n",
    "#env = MultiArmedBanditEnv(4)\n",
    "env.values = [1.0, 2.0, -1.0, 0.0, 0.0] # The last one is the initial state value\n",
    "\n",
    "A_gp = env.get_likelihood_dist()\n",
    "B_gp = env.get_transition_dist()\n",
    "A_gm = copy.deepcopy(A_gp) # make a copy of the true observation likelihood to initialize the observation model\n",
    "B_gm = copy.deepcopy(B_gp) # make a copy of the true transition likelihood to initialize the transition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistent observation sequences: 1816 (out of approx 13824)\n",
      " === Starting experiment === \n",
      " Reward condition: Right, Observation: [CENTER, No reward, Cue Left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:59<00:00,  1.79s/it, G=-1.8944035] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0] Action: [Move to CUE LOCATION]\n",
      "[Step 0] Observation: [CUE LOCATION,  No reward, Cue Right]\n",
      "[Step 1] Action: [Move to RIGHT ARM]\n",
      "[Step 1] Observation: [RIGHT ARM,  Reward!, Cue Left]\n",
      "[Step 2] Action: [Move to RIGHT ARM]\n",
      "[Step 2] Observation: [RIGHT ARM,  Reward!, Cue Right]\n",
      " === Starting experiment === \n",
      " Reward condition: Left, Observation: [CENTER, No reward, Cue Left]\n",
      "[Step 0] Action: [Move to CUE LOCATION]\n",
      "[Step 0] Observation: [CUE LOCATION,  No reward, Cue Left]\n",
      "[Step 1] Action: [Move to LEFT ARM]\n",
      "[Step 1] Observation: [LEFT ARM,  Reward!, Cue Left]\n",
      "[Step 2] Action: [Move to LEFT ARM]\n",
      "[Step 2] Observation: [LEFT ARM,  Reward!, Cue Right]\n",
      " === Starting experiment === \n",
      " Reward condition: Right, Observation: [CENTER, No reward, Cue Left]\n",
      "[Step 0] Action: [Move to CUE LOCATION]\n",
      "[Step 0] Observation: [CUE LOCATION,  No reward, Cue Right]\n",
      "[Step 1] Action: [Move to RIGHT ARM]\n",
      "[Step 1] Observation: [RIGHT ARM,  Reward!, Cue Left]\n",
      "[Step 2] Action: [Move to RIGHT ARM]\n",
      "[Step 2] Observation: [RIGHT ARM,  Reward!, Cue Right]\n",
      " === Starting experiment === \n",
      " Reward condition: Right, Observation: [CENTER, No reward, Cue Right]\n",
      "[Step 0] Action: [Move to CUE LOCATION]\n",
      "[Step 0] Observation: [CUE LOCATION,  No reward, Cue Right]\n",
      "[Step 1] Action: [Move to RIGHT ARM]\n",
      "[Step 1] Observation: [RIGHT ARM,  Reward!, Cue Left]\n",
      "[Step 2] Action: [Move to RIGHT ARM]\n",
      "[Step 2] Observation: [RIGHT ARM,  Reward!, Cue Right]\n",
      " === Starting experiment === \n",
      " Reward condition: Left, Observation: [CENTER, No reward, Cue Left]\n",
      "[Step 0] Action: [Move to CUE LOCATION]\n",
      "[Step 0] Observation: [CUE LOCATION,  No reward, Cue Left]\n",
      "[Step 1] Action: [Move to LEFT ARM]\n",
      "[Step 1] Observation: [LEFT ARM,  Loss!, Cue Right]\n",
      "[Step 2] Action: [Move to CENTER]\n",
      "[Step 2] Observation: [CENTER,  No reward, Cue Right]\n",
      " === Starting experiment === \n",
      " Reward condition: Left, Observation: [CENTER, No reward, Cue Right]\n",
      "[Step 0] Action: [Move to CUE LOCATION]\n",
      "[Step 0] Observation: [CUE LOCATION,  No reward, Cue Left]\n",
      "[Step 1] Action: [Move to LEFT ARM]\n",
      "[Step 1] Observation: [LEFT ARM,  Reward!, Cue Left]\n",
      "[Step 2] Action: [Move to LEFT ARM]\n",
      "[Step 2] Observation: [LEFT ARM,  Reward!, Cue Left]\n",
      " === Starting experiment === \n",
      " Reward condition: Right, Observation: [CENTER, No reward, Cue Right]\n",
      "[Step 0] Action: [Move to CUE LOCATION]\n",
      "[Step 0] Observation: [CUE LOCATION,  No reward, Cue Right]\n",
      "[Step 1] Action: [Move to RIGHT ARM]\n",
      "[Step 1] Observation: [RIGHT ARM,  Reward!, Cue Right]\n",
      "[Step 2] Action: [Move to RIGHT ARM]\n",
      "[Step 2] Observation: [RIGHT ARM,  Reward!, Cue Right]\n",
      " === Starting experiment === \n",
      " Reward condition: Right, Observation: [CENTER, No reward, Cue Right]\n",
      "[Step 0] Action: [Move to CUE LOCATION]\n",
      "[Step 0] Observation: [CUE LOCATION,  No reward, Cue Right]\n",
      "[Step 1] Action: [Move to RIGHT ARM]\n",
      "[Step 1] Observation: [RIGHT ARM,  Reward!, Cue Right]\n",
      "[Step 2] Action: [Move to RIGHT ARM]\n",
      "[Step 2] Observation: [RIGHT ARM,  Reward!, Cue Left]\n",
      " === Starting experiment === \n",
      " Reward condition: Right, Observation: [CENTER, No reward, Cue Left]\n",
      "[Step 0] Action: [Move to CUE LOCATION]\n",
      "[Step 0] Observation: [CUE LOCATION,  No reward, Cue Right]\n",
      "[Step 1] Action: [Move to RIGHT ARM]\n",
      "[Step 1] Observation: [RIGHT ARM,  Reward!, Cue Right]\n",
      "[Step 2] Action: [Move to RIGHT ARM]\n",
      "[Step 2] Observation: [RIGHT ARM,  Reward!, Cue Left]\n",
      " === Starting experiment === \n",
      " Reward condition: Right, Observation: [CENTER, No reward, Cue Right]\n",
      "[Step 0] Action: [Move to CUE LOCATION]\n",
      "[Step 0] Observation: [CUE LOCATION,  No reward, Cue Right]\n",
      "[Step 1] Action: [Move to RIGHT ARM]\n",
      "[Step 1] Observation: [RIGHT ARM,  Reward!, Cue Right]\n",
      "[Step 2] Action: [Move to RIGHT ARM]\n",
      "[Step 2] Observation: [RIGHT ARM,  Reward!, Cue Right]\n"
     ]
    }
   ],
   "source": [
    "T = 3 # number of timesteps: 2 or 3 for TMaze, 1 or more for MultiArmedBandit\n",
    "\n",
    "agent = EVAgent(A=A_gm, B=B_gm, time_horizon=T, env=env, policy_lr=10.0, policy_iterations=100)\n",
    "obs = env.reset() # reset the environment and get an initial observation\n",
    "\n",
    "print(f\"Consistent observation sequences: {len(agent.generate_consistent_observation_seqs())} (out of approx {len(agent.possible_observations) ** T})\")\n",
    "\n",
    "if isinstance(env, TMazeEnv):\n",
    "    agent.D[0] = utils.onehot(0, agent.num_states[0])\n",
    "    agent.C[1][1] = 1.0\n",
    "    agent.C[1][2] = -1.0\n",
    "    # these are useful for displaying read-outs during the loop over time\n",
    "    reward_conditions = [\"Right\", \"Left\"]\n",
    "    location_observations = ['CENTER','RIGHT ARM','LEFT ARM','CUE LOCATION']\n",
    "    reward_observations = ['No reward','Reward!','Loss!']\n",
    "    cue_observations = ['Cue Right','Cue Left']\n",
    "\n",
    "    for reps in range(10):\n",
    "        agent.reset()\n",
    "        obs = env.reset() # reset the environment and get an initial observation\n",
    "        msg = \"\"\" === Starting experiment === \\n Reward condition: {}, Observation: [{}, {}, {}]\"\"\"\n",
    "        print(msg.format(reward_conditions[env.reward_condition], location_observations[obs[0]], reward_observations[obs[1]], cue_observations[obs[2]]))\n",
    "        for t in range(T):\n",
    "            agent.infer_states(obs)\n",
    "            agent.infer_policies()\n",
    "            action = agent.sample_action()\n",
    "            msg = \"\"\"[Step {}] Action: [Move to {}]\"\"\"\n",
    "            print(msg.format(t, location_observations[int(action[0])]))\n",
    "            obs = env.step(action)\n",
    "            msg = \"\"\"[Step {}] Observation: [{},  {}, {}]\"\"\"\n",
    "            print(msg.format(t, location_observations[obs[0]], reward_observations[obs[1]], cue_observations[obs[2]]))\n",
    "\n",
    "elif isinstance(env, MultiArmedBanditEnv):\n",
    "    agent.D[0] = utils.onehot(env.INITIAL_STATE, agent.num_states[0])\n",
    "    agent.C[1][:] = env.values\n",
    "\n",
    "    print(f\"Initial observation: {obs}\")\n",
    "    for t in range(T):\n",
    "        agent.infer_states(obs)\n",
    "        agent.infer_policies()\n",
    "        action = agent.sample_action()\n",
    "        obs = env.step(action)\n",
    "        print(f\"Action: {np.array(action)}, Observation: {np.array(obs)}, Reward: {env.values[obs[0]]}\")\n",
    "\n",
    "else: \n",
    "    raise ValueError(\"Unknown environment type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[2.7379731e-04],\n",
       "       [3.6035053e-04],\n",
       "       [3.6035053e-04],\n",
       "       [9.9900550e-01]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy.observation_sequences\n",
    "agent.policy.policy_for_observations(((0,0,0),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
