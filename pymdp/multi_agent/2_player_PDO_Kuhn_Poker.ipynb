{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(1, 'C:/Users/davand/OneDrive/Documents/Programming/MAAIF/pymdp/')\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import copy\n",
    "from pymdp.multi_agent.kuhn_poker import KuhnPokerEnv\n",
    "from pymdp.pdo_agents.agent_gradient import PDOAgentGradient\n",
    "from pymdp.pdo_agents.full_policy import TabularSoftmaxPolicy\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "\n",
    "\n",
    "class PDOKuhnPokerAgent(PDOAgentGradient):\n",
    "    def __init__(self, A: np.ndarray, B: np.ndarray, player_idx: int, learning_rate: float = 0.01, beta: float = 1.0, time_horizon: int = 2):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.time_horizon = time_horizon\n",
    "        # self.env = KuhnPokerEnv()\n",
    "        \n",
    "        # Use the provided A and B matrices\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.C = self.initialize_preference_dist(player_idx)\n",
    "        \n",
    "        super().__init__(A=self.A, B=self.B, time_horizon=self.time_horizon, beta=self.beta)\n",
    "        \n",
    "        self.policy = self.initialize_policy()\n",
    "\n",
    "    def initialize_preference_dist(self, player_idx: int):\n",
    "        # preference for agent is the reward, i.e., negative of the pot if they lost, and positive if they won\n",
    "        # Define the preference matrix C\n",
    "        # The shape should be (num_observations, num_observations, num_actions)\n",
    "    \n",
    "        # list of three vectors: one of length num_cards, one of length num_actions, need third obs for pot observations - reward modality, \n",
    "        # may need extra level - Null level - predict 100% prob for this outcome until end of game\n",
    "\n",
    "\n",
    "        # Initialize C with zeros\n",
    "        C = np.zeros((self.num_cards, self.num_actions, self.num_rewards))\n",
    "        \n",
    "        # Define preferences based on the reward modality\n",
    "        for reward in range(self.num_rewards):\n",
    "            if reward == 0:  # -2 (big loss)\n",
    "                C[:, :, reward] = -2.0\n",
    "            elif reward == 1:  # -1 (small loss)\n",
    "                C[:, :, reward] = -1.0\n",
    "            elif reward == 2:  # 0 (neutral)\n",
    "                C[:, :, reward] = 0.0\n",
    "            elif reward == 3:  # 1 (small win)\n",
    "                C[:, :, reward] = 1.0\n",
    "            elif reward == 4:  # 2 (big win)\n",
    "                C[:, :, reward] = 2.0\n",
    "\n",
    "        # Normalize C using softmax\n",
    "        C = jax.nn.softmax(C, axis=-1)\n",
    "        \n",
    "        return C\n",
    "\n",
    "    def initialize_policy(self):\n",
    "        # Initialize the policy using TabularSoftmaxPolicy\n",
    "        observation_sequences = self.generate_consistent_observation_seqs()\n",
    "        return TabularSoftmaxPolicy(action_counts=self.num_controls, \n",
    "                                    observation_sequences=observation_sequences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_pdo_kuhn_poker(num_episodes: int = 10000):\n",
    "    env = KuhnPokerEnv()\n",
    "    A1 = copy.deepcopy(env.get_likelihood_dist(0))\n",
    "    B1 = copy.deepcopy(env.get_transition_dist(0))\n",
    "    A2 = copy.deepcopy(env.get_likelihood_dist(1))\n",
    "    B2 = copy.deepcopy(env.get_transition_dist(1))\n",
    "    agent1 = PDOKuhnPokerAgent(A=A1, B=B1, player_idx=0)\n",
    "    agent2 = PDOKuhnPokerAgent(A=A2, B=B2, player_idx=1)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if env.turn == 0 or env.turn == 2:\n",
    "                action = agent1.select_action(observation)\n",
    "            else:\n",
    "                action = agent2.select_action(observation)\n",
    "            \n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if done:\n",
    "                if env.turn == 0:\n",
    "                    agent1.update_policy(observation, action, reward)\n",
    "                    agent2.update_policy(next_observation, 'fold' if reward > 0 else 'call', -reward)\n",
    "                else:\n",
    "                    agent2.update_policy(observation, action, reward)\n",
    "                    agent1.update_policy(next_observation, 'fold' if reward < 0 else 'call', -reward)\n",
    "            else:\n",
    "                if env.turn == 1:  # Agent 1 just acted\n",
    "                    agent1.update_policy(observation, action, 0)\n",
    "                else:  # Agent 2 just acted\n",
    "                    agent2.update_policy(observation, action, 0)\n",
    "            \n",
    "            observation = next_observation\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            print(f\"Episode {episode}\")\n",
    "            print(\"Agent 1 policy:\", agent1.policy.table)\n",
    "            print(\"Agent 2 policy:\", agent2.policy.table)\n",
    "            print()\n",
    "\n",
    "    print(\"Final policies:\")\n",
    "    print(\"Agent 1:\", agent1.policy.table)\n",
    "    print(\"Agent 2:\", agent2.policy.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test\n",
      "[[1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 1.]]\n",
      "[[[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  ...\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  ...\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  ...\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  ...\n",
      "  [1. 1.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  ...\n",
      "  [0. 0.]\n",
      "  [1. 1.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  ...\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [1. 1.]]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BranchingAgent.__init__() missing 1 required positional argument: 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplay_pdo_kuhn_poker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mplay_pdo_kuhn_poker\u001b[1;34m(num_episodes)\u001b[0m\n\u001b[0;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m KuhnPokerEnv()\n\u001b[0;32m      3\u001b[0m A, B \u001b[38;5;241m=\u001b[39m initialize_kuhn_poker_matrices()\n\u001b[1;32m----> 4\u001b[0m agent1 \u001b[38;5;241m=\u001b[39m \u001b[43mPDOKuhnPokerAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m agent2 \u001b[38;5;241m=\u001b[39m PDOKuhnPokerAgent(A\u001b[38;5;241m=\u001b[39mA, B\u001b[38;5;241m=\u001b[39mB)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m, in \u001b[0;36mPDOKuhnPokerAgent.__init__\u001b[1;34m(self, A, B, learning_rate, beta, time_horizon)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB \u001b[38;5;241m=\u001b[39m B\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_preference_dist()\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_horizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_horizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_policy()\n",
      "\u001b[1;31mTypeError\u001b[0m: BranchingAgent.__init__() missing 1 required positional argument: 'env'"
     ]
    }
   ],
   "source": [
    "play_pdo_kuhn_poker()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
