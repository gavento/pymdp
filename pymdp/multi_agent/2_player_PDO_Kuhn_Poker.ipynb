{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax.numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkuhn_poker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KuhnPokerEnv\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpdo_agents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_gradient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PDOAgentGradient\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpdo_agents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfull_policy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabularSoftmaxPolicy\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple, Dict, List\n",
      "File \u001b[1;32mC:\\Users/davand/OneDrive/Documents/Programming/MAAIF/pymdp/pymdp\\pdo_agents\\__init__.py:10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\" A module with PDO-based agents\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m__author__: Tomáš Gavenčiak\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m agent_base, agent_direct, agent_gradient, common\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_direct\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EVAgentDirect, PDOAgentDirect\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_gradient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EVAgentGradient, PDOAgentGradient\n",
      "File \u001b[1;32mC:\\Users/davand/OneDrive/Documents/Programming/MAAIF/pymdp/pymdp\\pdo_agents\\agent_base.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Env\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jax.numpy'"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "sys.path.insert(1, 'C:/Users/davand/OneDrive/Documents/Programming/MAAIF/pymdp/pymdp')\n",
    "import numpy as np\n",
    "import jax\n",
    "# import jax.numpy as jnp\n",
    "import copy\n",
    "from kuhn_poker import KuhnPokerEnv\n",
    "from pdo_agents.agent_gradient import PDOAgentGradient\n",
    "from pdo_agents.full_policy import TabularSoftmaxPolicy\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "class PDOKuhnPokerAgent(PDOAgentGradient):\n",
    "    def __init__(self, A: np.ndarray, B: np.ndarray, env: KuhnPokerEnv, learning_rate: float = 0.01, beta: float = 1.0, time_horizon: int = 2):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.time_horizon = time_horizon\n",
    "        # self.env = KuhnPokerEnv()\n",
    "        \n",
    "        # Use the provided A and B matrices\n",
    "        self.A = copy.deepcopy(env.get_likelihood_dist())\n",
    "        self.B = copy.deepcopy(env.get_transition_dist())\n",
    "        self.C = self.initialize_preference_dist()\n",
    "        \n",
    "        super().__init__(A=self.A, B=self.B, time_horizon=self.time_horizon, \n",
    "                         env=self.env, beta=self.beta)\n",
    "        \n",
    "        self.policy = self.initialize_policy()\n",
    "\n",
    "    def initialize_preference_dist(self):\n",
    "        # preference for agent is the reward, i.e., negative of the pot if they lost, and positive if they won\n",
    "        # Define the preference matrix C\n",
    "        # The shape should be (num_observations, num_observations, num_actions)\n",
    "        num_observations = self.A.shape[0]\n",
    "        num_actions = self.B.shape[-1]\n",
    "        \n",
    "        # Initialize C with zeros\n",
    "        C = np.zeros((num_observations, num_observations, num_actions))\n",
    "        \n",
    "        # Define preferences based on game outcomes\n",
    "        # Winning states have positive preference, losing states have negative preference\n",
    "        # The magnitude can be set to the pot size (1 for small pot, 2 for big pot)\n",
    "        \n",
    "        # Winning states (getting the pot)\n",
    "        C[-2, :, :] = 1  # Small pot win\n",
    "        C[-1, :, :] = 2  # Big pot win\n",
    "        \n",
    "        # Losing states (losing the pot)\n",
    "        C[:, -2, :] = -1  # Small pot loss\n",
    "        C[:, -1, :] = -2  # Big pot loss\n",
    "        \n",
    "        return C\n",
    "\n",
    "    def initialize_policy(self):\n",
    "        # Initialize the policy using TabularSoftmaxPolicy\n",
    "        observation_sequences = self.generate_consistent_observation_seqs()\n",
    "        return TabularSoftmaxPolicy(action_counts=self.num_controls, \n",
    "                                    observation_sequences=observation_sequences)\n",
    "\n",
    "    def update_policy(self, observation: Dict, action: str, reward: float):\n",
    "        # Update the policy using gradient descent\n",
    "        grad = jax.grad(self.G)(self.policy)\n",
    "        self.policy.table -= self.learning_rate * grad.table\n",
    "\n",
    "    def select_action(self, observation: Dict) -> str:\n",
    "        obs_seq = self.observation_to_sequence(observation)\n",
    "        action_probs = self.policy.policy_for_observations(obs_seq)\n",
    "        action = jax.random.choice(jax.random.PRNGKey(0), 2, p=action_probs)\n",
    "        return 'check' if action == 0 else 'bet'\n",
    "\n",
    "    def observation_to_sequence(self, observation: Dict) -> Tuple:\n",
    "        # Convert the Kuhn Poker observation to a sequence compatible with the policy\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_pdo_kuhn_poker(num_episodes: int = 10000):\n",
    "    env = KuhnPokerEnv()\n",
    "    A, B = initialize_kuhn_poker_matrices()\n",
    "    agent1 = PDOKuhnPokerAgent(A=A, B=B, env=env)\n",
    "    agent2 = PDOKuhnPokerAgent(A=A, B=B, env=env)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if env.turn == 0 or env.turn == 2:\n",
    "                action = agent1.select_action(observation)\n",
    "            else:\n",
    "                action = agent2.select_action(observation)\n",
    "            \n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if done:\n",
    "                if env.turn == 0:\n",
    "                    agent1.update_policy(observation, action, reward)\n",
    "                    agent2.update_policy(next_observation, 'fold' if reward > 0 else 'call', -reward)\n",
    "                else:\n",
    "                    agent2.update_policy(observation, action, reward)\n",
    "                    agent1.update_policy(next_observation, 'fold' if reward < 0 else 'call', -reward)\n",
    "            else:\n",
    "                if env.turn == 1:  # Agent 1 just acted\n",
    "                    agent1.update_policy(observation, action, 0)\n",
    "                else:  # Agent 2 just acted\n",
    "                    agent2.update_policy(observation, action, 0)\n",
    "            \n",
    "            observation = next_observation\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            print(f\"Episode {episode}\")\n",
    "            print(\"Agent 1 policy:\", agent1.policy.table)\n",
    "            print(\"Agent 2 policy:\", agent2.policy.table)\n",
    "            print()\n",
    "\n",
    "    print(\"Final policies:\")\n",
    "    print(\"Agent 1:\", agent1.policy.table)\n",
    "    print(\"Agent 2:\", agent2.policy.table)\n",
    "\n",
    "\n",
    "def initialize_kuhn_poker_matrices():\n",
    "    # Define the state space\n",
    "    # States: (player1_card, player2_card, betting_round, last_action)\n",
    "    # Cards: Jack (0), Queen (1), King (2)\n",
    "    # Betting rounds: 0 (initial), 1 (after first action)\n",
    "    # Last action: None (-1), Check (0), Bet (1)\n",
    "    num_states = 3 * 3 * 2 * 3  # 54 states\n",
    "\n",
    "    # Define the observation space\n",
    "    # Observations: (own_card, betting_round, last_action)\n",
    "    num_obs = 3 * 2 * 3  # 18 observations\n",
    "\n",
    "    # Initialize A matrix (observation likelihood)\n",
    "    A = np.zeros((num_obs, num_states))\n",
    "    for s in range(num_states):\n",
    "        p1_card, p2_card, betting_round, last_action = np.unravel_index(s, (3, 3, 2, 3))\n",
    "        for o in range(num_obs):\n",
    "            obs_card, obs_betting_round, obs_last_action = np.unravel_index(o, (3, 2, 3))\n",
    "            if obs_card == p1_card and obs_betting_round == betting_round and obs_last_action == last_action:\n",
    "                A[o, s] = 1\n",
    "\n",
    "    # Normalize A matrix\n",
    "    A /= A.sum(axis=0, keepdims=True)\n",
    "\n",
    "    # Initialize B matrix (transition probabilities)\n",
    "    num_actions = 2  # Check/Call (0) or Bet/Raise (1)\n",
    "    B = np.zeros((num_states, num_states, num_actions))\n",
    "\n",
    "    for s in range(num_states):\n",
    "        p1_card, p2_card, betting_round, last_action = np.unravel_index(s, (3, 3, 2, 3))\n",
    "        \n",
    "        if betting_round == 0:\n",
    "            for a in range(num_actions):\n",
    "                next_s = np.ravel_multi_index((p1_card, p2_card, 1, a), (3, 3, 2, 3))\n",
    "                B[next_s, s, a] = 1\n",
    "        elif betting_round == 1 and last_action != -1:\n",
    "            # Game ends, stay in the same state\n",
    "            B[s, s, :] = 1\n",
    "\n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KuhnPokerEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplay_pdo_kuhn_poker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m, in \u001b[0;36mplay_pdo_kuhn_poker\u001b[1;34m(num_episodes)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay_pdo_kuhn_poker\u001b[39m(num_episodes: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mKuhnPokerEnv\u001b[49m()\n\u001b[0;32m      3\u001b[0m     A, B \u001b[38;5;241m=\u001b[39m initialize_kuhn_poker_matrices()\n\u001b[0;32m      4\u001b[0m     agent1 \u001b[38;5;241m=\u001b[39m PDOKuhnPokerAgent(A\u001b[38;5;241m=\u001b[39mA, B\u001b[38;5;241m=\u001b[39mB, env\u001b[38;5;241m=\u001b[39menv)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KuhnPokerEnv' is not defined"
     ]
    }
   ],
   "source": [
    "play_pdo_kuhn_poker()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
