{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(1, 'C:/Users/davand/OneDrive/Documents/Programming/MAAIF/pymdp/')\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import copy\n",
    "from pymdp.multi_agent.kuhn_poker import KuhnPokerEnv\n",
    "from pymdp.pdo_agents.agent_gradient import PDOAgentGradient\n",
    "from pymdp.pdo_agents.full_policy import TabularSoftmaxPolicy\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "\n",
    "\n",
    "class PDOKuhnPokerAgent(PDOAgentGradient):\n",
    "    def __init__(self, A: np.ndarray, B: np.ndarray, learning_rate: float = 0.01, beta: float = 1.0, time_horizon: int = 2):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.time_horizon = time_horizon\n",
    "        # self.env = KuhnPokerEnv()\n",
    "        \n",
    "        # Use the provided A and B matrices\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.C = self.initialize_preference_dist()\n",
    "        \n",
    "        super().__init__(A=self.A, B=self.B, time_horizon=self.time_horizon, beta=self.beta)\n",
    "        \n",
    "        self.policy = self.initialize_policy()\n",
    "\n",
    "    def initialize_preference_dist(self):\n",
    "        # preference for agent is the reward, i.e., negative of the pot if they lost, and positive if they won\n",
    "        # Define the preference matrix C\n",
    "        # The shape should be (num_observations, num_observations, num_actions)\n",
    "        num_observations = self.A.shape[0]\n",
    "        num_actions = self.B.shape[-1]\n",
    "    \n",
    "        # list of two vectors: one of length num_cards, one of length num_actions, need third obs for pot observations - reward modality, may need extra level - Null level - predict 100% prob for this outcome until end of game\n",
    "\n",
    "\n",
    "        # Initialize C with zeros\n",
    "        C = np.zeros((num_observations, num_observations, num_actions))\n",
    "        \n",
    "        # Define preferences based on game outcomes\n",
    "        # Winning states have positive preference, losing states have negative preference\n",
    "        # The magnitude can be set to the pot size (1 for small pot, 2 for big pot)\n",
    "        \n",
    "        # Winning states (getting the pot)\n",
    "        C[-2, :, :] = 1  # Small pot win\n",
    "        C[-1, :, :] = 2  # Big pot win\n",
    "        \n",
    "        # Losing states (losing the pot)\n",
    "        C[:, -2, :] = -1  # Small pot loss\n",
    "        C[:, -1, :] = -2  # Big pot loss\n",
    "        \n",
    "        return C\n",
    "\n",
    "    def initialize_policy(self):\n",
    "        # Initialize the policy using TabularSoftmaxPolicy\n",
    "        observation_sequences = self.generate_consistent_observation_seqs()\n",
    "        return TabularSoftmaxPolicy(action_counts=self.num_controls, \n",
    "                                    observation_sequences=observation_sequences)\n",
    "\n",
    "    def update_policy(self, observation: Dict, action: str, reward: float):\n",
    "        # Update the policy using gradient descent\n",
    "        grad = jax.grad(self.G)(self.policy)\n",
    "        self.policy.table -= self.learning_rate * grad.table\n",
    "\n",
    "    def select_action(self, observation: Dict) -> str:\n",
    "        obs_seq = self.observation_to_sequence(observation)\n",
    "        action_probs = self.policy.policy_for_observations(obs_seq)\n",
    "        action = jax.random.choice(jax.random.PRNGKey(0), 2, p=action_probs)\n",
    "        return 'check' if action == 0 else 'bet'\n",
    "\n",
    "    def observation_to_sequence(self, observation: Dict) -> Tuple:\n",
    "        # Convert the Kuhn Poker observation to a sequence compatible with the policy\n",
    "        card = observation['card']\n",
    "        history = observation['history']\n",
    "        \n",
    "        # Map card to integer (Jack: 0, Queen: 1, King: 2)\n",
    "        card_mapping = {'J': 0, 'Q': 1, 'K': 2}\n",
    "        card_int = card_mapping[card]\n",
    "        \n",
    "        # Convert history to a sequence of integers\n",
    "        # 'c' for check: 0, 'b' for bet: 1\n",
    "        history_sequence = tuple(0 if action == 'c' else 1 for action in history)\n",
    "        \n",
    "        # Combine card and history into a single sequence\n",
    "        return (card_int,) + history_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_pdo_kuhn_poker(num_episodes: int = 10000):\n",
    "    env = KuhnPokerEnv()\n",
    "    A, B = initialize_kuhn_poker_matrices()\n",
    "    agent1 = PDOKuhnPokerAgent(A=A, B=B)\n",
    "    agent2 = PDOKuhnPokerAgent(A=A, B=B)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if env.turn == 0 or env.turn == 2:\n",
    "                action = agent1.select_action(observation)\n",
    "            else:\n",
    "                action = agent2.select_action(observation)\n",
    "            \n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if done:\n",
    "                if env.turn == 0:\n",
    "                    agent1.update_policy(observation, action, reward)\n",
    "                    agent2.update_policy(next_observation, 'fold' if reward > 0 else 'call', -reward)\n",
    "                else:\n",
    "                    agent2.update_policy(observation, action, reward)\n",
    "                    agent1.update_policy(next_observation, 'fold' if reward < 0 else 'call', -reward)\n",
    "            else:\n",
    "                if env.turn == 1:  # Agent 1 just acted\n",
    "                    agent1.update_policy(observation, action, 0)\n",
    "                else:  # Agent 2 just acted\n",
    "                    agent2.update_policy(observation, action, 0)\n",
    "            \n",
    "            observation = next_observation\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            print(f\"Episode {episode}\")\n",
    "            print(\"Agent 1 policy:\", agent1.policy.table)\n",
    "            print(\"Agent 2 policy:\", agent2.policy.table)\n",
    "            print()\n",
    "\n",
    "    print(\"Final policies:\")\n",
    "    print(\"Agent 1:\", agent1.policy.table)\n",
    "    print(\"Agent 2:\", agent2.policy.table)\n",
    "\n",
    "\n",
    "def initialize_kuhn_poker_matrices():\n",
    "    # Define the state space\n",
    "    # States: (player1_card, player2_card, betting_round, last_action)\n",
    "    # Cards: Jack (0), Queen (1), King (2)\n",
    "    # Betting rounds: 0 (initial), 1 (after first action)\n",
    "    # Last action: None (-1), Check (0), Bet (1)\n",
    "    num_states = 3 * 3 * 2 * 3  # 54 states\n",
    "\n",
    "    # Define the observation space\n",
    "    # Observations: (own_card, betting_round, last_action)\n",
    "    num_obs = 3 * 2 * 3  # 18 observations\n",
    "\n",
    "    # Initialize A matrix (observation likelihood)\n",
    "    A = np.zeros((num_obs, num_states))\n",
    "    for s in range(num_states):\n",
    "        p1_card, p2_card, betting_round, last_action = np.unravel_index(s, (3, 3, 2, 3))\n",
    "        for o in range(num_obs):\n",
    "            obs_card, obs_betting_round, obs_last_action = np.unravel_index(o, (3, 2, 3))\n",
    "            if obs_card == p1_card and obs_betting_round == betting_round and obs_last_action == last_action:\n",
    "                A[o, s] = 1\n",
    "\n",
    "    # Normalize A matrix\n",
    "    A /= A.sum(axis=0, keepdims=True)\n",
    "\n",
    "    # Initialize B matrix (transition probabilities)\n",
    "    num_actions = 2  # Check/Call (0) or Bet/Raise (1)\n",
    "    B = np.zeros((num_states, num_states, num_actions))\n",
    "\n",
    "    for s in range(num_states):\n",
    "        p1_card, p2_card, betting_round, last_action = np.unravel_index(s, (3, 3, 2, 3))\n",
    "        \n",
    "        if betting_round == 0:\n",
    "            for a in range(num_actions):\n",
    "                next_s = np.ravel_multi_index((p1_card, p2_card, 1, a), (3, 3, 2, 3))\n",
    "                B[next_s, s, a] = 1\n",
    "        elif betting_round == 1 and last_action != -1:\n",
    "            # Game ends, stay in the same state\n",
    "            B[s, s, :] = 1\n",
    "    \n",
    "    print(A)\n",
    "    print(B)\n",
    "\n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test\n",
      "[[1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 1.]]\n",
      "[[[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  ...\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  ...\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  ...\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  ...\n",
      "  [1. 1.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  ...\n",
      "  [0. 0.]\n",
      "  [1. 1.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  ...\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [1. 1.]]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BranchingAgent.__init__() missing 1 required positional argument: 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplay_pdo_kuhn_poker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mplay_pdo_kuhn_poker\u001b[1;34m(num_episodes)\u001b[0m\n\u001b[0;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m KuhnPokerEnv()\n\u001b[0;32m      3\u001b[0m A, B \u001b[38;5;241m=\u001b[39m initialize_kuhn_poker_matrices()\n\u001b[1;32m----> 4\u001b[0m agent1 \u001b[38;5;241m=\u001b[39m \u001b[43mPDOKuhnPokerAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m agent2 \u001b[38;5;241m=\u001b[39m PDOKuhnPokerAgent(A\u001b[38;5;241m=\u001b[39mA, B\u001b[38;5;241m=\u001b[39mB)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m, in \u001b[0;36mPDOKuhnPokerAgent.__init__\u001b[1;34m(self, A, B, learning_rate, beta, time_horizon)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB \u001b[38;5;241m=\u001b[39m B\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_preference_dist()\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_horizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_horizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_policy()\n",
      "\u001b[1;31mTypeError\u001b[0m: BranchingAgent.__init__() missing 1 required positional argument: 'env'"
     ]
    }
   ],
   "source": [
    "play_pdo_kuhn_poker()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
